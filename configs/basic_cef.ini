# Sample config file for running DAS in the container
# Based heavily on das_decennial/configs/run.ini
# and das_decennial/configs/production/pl94

[logging]
logfilename = DAS
loglevel = INFO
logfolder = logs

[ENVIRONMENT]
das_framework_version = 1.1.0
GRB_ISV_NAME = Standalone
cluster = EDU
GRB_APP_NAME = DAS
GRB_Env3 = 0
GRB_Env4 =

[geodict]:
aian_areas = Legal_Federally_Recognized_American_Indian_Area,
    American_Indian_Joint_Use_Area,
    Hawaiian_Home_Land,
    Alaska_Native_Village_Statistical_Area,
    State_Recognized_Legal_American_Indian_Area,
    Oklahoma_Tribal_Statistical_Area,
    Joint_Use_Oklahoma_Tribal_Statistical_Area
geocode_length = 16
geolevel_leng = 16,12,11,5,2,0
geolevel_names = Block,Block_Group,Tract,County,State,US
geo_bottomlevel = Block
geo_path = 
geo_toplevel = 
ignore_gqs_in_block_groups = False
target_das_aian_areas = True
spine = opt_spine

[setup]
setup = programs.das_setup.DASDecennialSetup
spark.name = DAS_CEF_TEST
#Error , only writes to log if there is an error (INFO, DEBUG, )
spark.loglevel = ERROR

[reader]
tables = Unit Person
constraint_tables = Unit
privacy_table = Person

reader = programs.reader.table_reader.DASDecennialReader
readerpartitionlen = 14
delimiter = |
grfc_path = $HOME/das_files/grfc_combined.csv
header = False
numreaderpartitions = 20

person.class = programs.reader.cef_2020.cef_2020_dhcp_reader.CEF2020PersonsTable
person.generated_module = programs.reader.cef_2020.cef_validator_classes
person.generated_table = CEF20_PER
person.geography = geocode
person.histogram = hhgq votingage hispanic cenrace_das
person.path = $HOME/das_files/converted_synth_pop.cef
person.recoder = programs.reader.cef_2020.cef_2020_dhcp_reader.PL94_2020_recoder
person.recode_variables = hhgq votingage hispanic cenrace_das

unit.class = programs.reader.cef_2020.cef_2020_dhcp_reader.CEF2020DHCPUnitTable
unit.generated_module = programs.reader.cef_2020.cef_validator_classes
unit.generated_table = CEF20_UNIT
unit.geography = geocode
unit.histogram = hhgq_unit_simple_recoded
unit.path = $HOME/das_files/converted_synth_unit.cef
unit.recoder = programs.reader.cef_2020.cef_2020_dhcp_reader.PL94_2020_Unit_recoder
unit.recode_variables = hhgq_unit_simple_recoded
validate_input_data_constraints = True

hhgq = qgqtyp
hhgq.legal = 0-7
hhgq.type = int

votingage = qage
votingage.legal = 0-1
votingage.type = int

hispanic = cenhisp
hispanic.legal = 0-1
hispanic.type = int

cenrace_das = cenrace
cenrace_das.legal = 0-62
cenrace_das.type = int

hhgq_unit_simple_recoded = qgqtyp
hhgq_unit_simple_recoded.legal = 0-7
hhgq_unit_simple_recoded.type = int

[engine]
check_budget = off
delete_raw = 0
engine = programs.engine.topdown_engine.TopdownEngine
noisy_measurements_postfix = NMF10_PER_US
reload_noisy = 0
save_noisy = TRUE

[schema]
schema = PL94_2020_SCHEMA

[budget]
approx_dp_delta = 1e-10
dp_mechanism = discrete_gaussian_mechanism
geolevel_budget_prop = 104/4099, 1440/4099, 447/4099, 687/4099, 1256/4099, 165/4099
global_scale = 339/542
only_dyadic_rationals = False
privacy_framework = zcdp
query_ordering = Strategy1b_ST_CTY_TR_BG_isoTot_Ordering_dsepJune3
strategy = ProductionCandidate20210527US_mult8_add02_dsepJune3

; [workload]
; workload: old_pl94_manual_workload

[constraints]
theInvariants.Block = gqhh_vect, gqhh_tot
theInvariants.State = tot
theConstraints.Block = hhgq_total_lb, hhgq_total_ub, nurse_nva_0
# US and State constraints are different for US and PR, and are defined explicitly in person_US.ini and person_PR.ini
# Explicitly setting the State and US constraints here because they differ from PR
theConstraints.State = hhgq_total_lb, hhgq_total_ub
theConstraints.US = total
minimalSchema = hhgq

[writer]
certificate_name = A very precise data set
certificate_person1 = Ben Bitdiddle
certificate_person2 = Alyssa P. Hacker
certificate_suffix = .certificate.pdf
certificate_title = Certificate of Disclosure Avoidance
certificate_title1 = Novice Programmer
certificate_title2 = Supervisor
drb_clearance_number = CBDRB-FY21-DSEP-005
classification_level = C_U_I//SP-CENS - Title 13 protected data

keep_attrs = geocode, syn, unit_syn, _invar, _cons, raw, raw_housing
multiwriter_writers = BlockNodeDicts, MDFPL942020
num_parts = 10
output_datafile_name = MDF10_PER_US

stats_dir = $HOME/das_files/upload
output_path = $HOME/das_files/results

upload_logfile = 0
writer = programs.writer.multi_writer.MultiWriter
write_metadata = 1
overwrite_flag = 1
produce_flag = 1
s3cat = 1
s3cat_suffix = .txt
s3cat_verbose = 0
save_git_commit = 1

[validator]
validator = programs.stub_validator.validator

[takedown]
takedown = programs.takedown.takedown
delete_output = 0

[experiment]
run_experiment_flag: 0

[error_metrics]
population_cutoff = 500

error_metrics = programs.metrics.accuracy_metrics.AbstractDASErrorMetrics
calculate_binned_query_errors = True
calculate_per_query_quantile_errors = True
calculate_per_query_quantile_signed_errors = True
l1_relative_error_geolevels = Place, Block_Group, OSE
l1_relative_error_queries = cenrace_7lev_two_comb * hispanic, gqlevels
print_blau_quintile_errors = True
print_8_cell_cenrace_hisp_errors = True
print_place_mcd_ose_bg_l1_error_on_total_pop = True
print_aians_l1_error_on_total_pop = True

[gurobi]
gurobi_path = /usr/local/gurobi911/linux64/${PYTHON_VERSION}_utf32/
gurobi_logfile_name = gurobi.log
gurobi_lic = ${HOME}/gurobi.lic
barconvtol = 0.0
bariterlimit = 1000
dataindnpass_tolerancetype = opt_tol
feasibilitytol = 1e-7
l2_acceptable_statuses = OPTIMAL, SUBOPTIMAL, ITERATION_LIMIT
l2_grb_algorithm = -1
l2_grb_presolve = -1
l2_grb_presparsify = -1
l2_optimization_approach = DataIndUserSpecifiedQueriesNPass
l2_suboptimal_allowed = False
method = -1
numericfocus = 3
optimalitytol = 1e-6
opt_tol_slack = 0.1
outputflag = 1
presolve = -1
python_presolve = 1
rounder_acceptable_statuses = OPTIMAL
rounder_optimization_approach = MultipassRounder
seq_optimization_approach = L2PlusRounderWithBackup_interleaved
record_gurobi_stats = True
record_cpu_stats: False
print_gurobi_stats: True
gurobi_lic_create: False

# Control the number of threads used by Gurobi
threads = 96
# Threads for the top-geolevel
threads_root2root = 96
# Threads for each geolevel (if not top geolevel)
threads_state = 96
threads_county = 32
# Not used, commenting out
# threads_tract_group = 96
threads_tract = 16
threads_block_group = 8
threads_block = 4

[stats]
heartbeat_frequency = 0
# Notify when GC is run on the master node
notify_gc_master = 0

[monitoring]
# Heartbeat just tells the user and the dashboard that we are alive
print_heartbeat = False
print_heartbeat_frequency = 0
send_stacktrace = False
heartbeat_frequency = 0

# Do we log to the dashboard on successful token acquisitions and retries?
# notifying that we got a token is just for debugging; this will typically be false
notify_dashboard_gurobi_success = false

# notifying that we had to retry is a problem; typically this will be true
notify_dashboard_gurobi_retry = false

# Notifications are about the current execution of the optimizer.
# It's collected from syslog.
notification_frequency = 0

[alert]
# Print this message when the system starts up
message = Hello world!
